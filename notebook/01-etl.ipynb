{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "zip_file_path = os.path.join(os.getcwd(), \"..\", \"data\", \"raw\", \"airports-database.zip\")\n",
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "    with zip_ref.open(\"airports-database.csv\") as csv_file:\n",
    "        df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# API keys do arquivo .env\n",
    "load_dotenv()\n",
    "airportdb_key = os.getenv(\"AIRPORT_DB\")\n",
    "weatherbit_key = os.getenv(\"WEATHERBIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType, DateType\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "\n",
    "# Inicializando a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\n",
    "\n",
    "def get_airport_data(airport_code, api_key):\n",
    "    \"\"\"\n",
    "    Obtém informações sobre um aeroporto a partir de seu código usando a API do AirportDB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    airport_code : str\n",
    "        Código do aeroporto (ex.: 'JFK').\n",
    "    api_key : str\n",
    "        Token da API para autenticação.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dados do aeroporto em formato JSON.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> airport_data = get_airport_data('JFK', 'seu_api_token_aqui')\n",
    "    >>> print(airport_data)\n",
    "    {'ident': 'KJFK', 'type': 'large_airport', 'name': 'John F Kennedy International Airport', ...}\n",
    "    \"\"\"\n",
    "    url = f\"https://airportdb.io/api/v1/airport/K{airport_code}?apiToken={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "\n",
    "def get_weather_history(lat, lon, start_date, end_date, api_key):\n",
    "    \"\"\"\n",
    "    Obtém dados históricos de clima para uma localização específica usando a API do Weatherbit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat : float\n",
    "        Latitude da localização (ex.: 40.7128).\n",
    "    lon : float\n",
    "        Longitude da localização (ex.: -74.0060).\n",
    "    start_date : str\n",
    "        Data de início no formato 'YYYY-MM-DD'.\n",
    "    end_date : str\n",
    "        Data de término no formato 'YYYY-MM-DD'.\n",
    "    api_key : str\n",
    "        Chave da API para autenticação.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dados do clima em formato JSON.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> weather_data = get_weather_history(40.7128, -74.0060, '2023-01-01', '2023-01-02', 'seu_api_key_aqui')\n",
    "    >>> print(weather_data)\n",
    "    {'data': 'city_id': '5128581', 'city_name': 'New York City', 'country_code': 'US', ...}\n",
    "    \"\"\"\n",
    "    url = \"https://api.weatherbit.io/v2.0/history/daily\"\n",
    "    params = {\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'key': api_key\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "\n",
    "def enrich_airport_data(spark, df, api_token):\n",
    "    \"\"\"\n",
    "    Enriquece um DataFrame do PySpark com informações de latitude e longitude dos aeroportos.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Sessão Spark.\n",
    "    df : pyspark.sql.DataFrame\n",
    "        DataFrame contendo a coluna 'airport_cod' com os códigos dos aeroportos.\n",
    "    api_token : str\n",
    "        Token da API para autenticação.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        DataFrame enriquecido com as colunas 'latitude_deg' e 'longitude_deg'.\n",
    "    \"\"\"\n",
    "    unique_codes = df.select(\"airport_cod\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "\n",
    "    for code in unique_codes:\n",
    "        try:\n",
    "            airport_data = get_airport_data(code, api_token)\n",
    "            latitudes.append((code, airport_data.get('latitude_deg'), airport_data.get('longitude_deg')))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao obter dados do aeroporto {code}: {e}\")\n",
    "            latitudes.append((code, None, None))\n",
    "\n",
    "    latlon_df = spark.createDataFrame(latitudes, [\"airport_cod\", \"latitude_deg\", \"longitude_deg\"])\n",
    "    return df.join(latlon_df, on=\"airport_cod\", how=\"left\")\n",
    "\n",
    "\n",
    "def get_unique_airports_by_date(df, airport_col):\n",
    "    \"\"\"\n",
    "    Obtém um DataFrame do PySpark contendo aeroportos únicos por data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        DataFrame contendo os dados de voos.\n",
    "    airport_col : str\n",
    "        Campo de origem ou destino ('origin' ou 'dest').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        DataFrame contendo aeroportos únicos e suas respectivas datas.\n",
    "    \"\"\"\n",
    "    df = df.select(airport_col, \"time_hour\")\n",
    "    df = df.withColumn(\"date\", F.to_date(\"time_hour\", \"yyyy-MM-dd\"))\n",
    "    df = df.select(F.col(airport_col).alias(\"airport_cod\"), \"date\").distinct()\n",
    "    return df\n",
    "\n",
    "\n",
    "# variável de controle para interromper o processamento\n",
    "stop_execution = [False]\n",
    "\n",
    "\n",
    "def enrich_weather_data(spark, df, api_key):\n",
    "    \"\"\"\n",
    "    Enriquece um DataFrame do PySpark com informações de velocidade do vento para as coordenadas do aeroporto.\n",
    "    Interrompe a execução ao receber o erro 429.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Sessão Spark.\n",
    "    df : pyspark.sql.DataFrame\n",
    "        DataFrame contendo colunas de data, latitude e longitude do aeroporto.\n",
    "    api_key : str\n",
    "        Chave da API para autenticação.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        DataFrame enriquecido com a coluna 'wind_spd', contendo os dados obtidos até o momento.\n",
    "    \"\"\"\n",
    "\n",
    "    def fetch_weather(lat, lon, date):\n",
    "        if lat is not None and lon is not None:\n",
    "            if stop_execution[0]:\n",
    "                return None  # evita fazer requisições adicionais após erro 429\n",
    "            try:\n",
    "                start_date = date.strftime(\"%Y-%m-%d\")\n",
    "                end_date = (date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # faz a chamada para a API de clima\n",
    "                weather_data = get_weather_history(lat, lon, start_date, end_date, api_key)\n",
    "                return weather_data[\"data\"][0][\"wind_spd\"]\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if \"429\" in str(e):\n",
    "                    print(\"Erro 429 recebido. Parando novas requisições.\")\n",
    "                    stop_execution[0] = True  # atualiza para parar futuras requisições\n",
    "                return None  # retorna None para a UDF continuar o processamento restante\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    fetch_weather_udf = F.udf(fetch_weather, DoubleType())\n",
    "\n",
    "    # enriquecendo o DataFrame com a coluna 'wind_spd'\n",
    "    df = df.withColumn(\"wind_spd\", fetch_weather_udf(F.col(\"latitude_deg\"), F.col(\"longitude_deg\"), F.col(\"date\")))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plbalmeida/picpay-case-machine-learning-engineer/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 11:30:35 WARN TaskSetManager: Stage 75 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:31:00 WARN TaskSetManager: Stage 76 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao obter dados do aeroporto PSE: 404 Client Error: Not Found for url: https://airportdb.io/api/v1/airport/KPSE?apiToken=e1974b7ce50088c4bf8c096e8722010381ebe6e8aef06041cb3cb2901699c023addbc1da0ae7ff96a58434dad6b76639\n",
      "Erro ao obter dados do aeroporto HNL: 404 Client Error: Not Found for url: https://airportdb.io/api/v1/airport/KHNL?apiToken=e1974b7ce50088c4bf8c096e8722010381ebe6e8aef06041cb3cb2901699c023addbc1da0ae7ff96a58434dad6b76639\n",
      "Erro ao obter dados do aeroporto SJU: 404 Client Error: Not Found for url: https://airportdb.io/api/v1/airport/KSJU?apiToken=e1974b7ce50088c4bf8c096e8722010381ebe6e8aef06041cb3cb2901699c023addbc1da0ae7ff96a58434dad6b76639\n",
      "Erro ao obter dados do aeroporto BQN: 404 Client Error: Not Found for url: https://airportdb.io/api/v1/airport/KBQN?apiToken=e1974b7ce50088c4bf8c096e8722010381ebe6e8aef06041cb3cb2901699c023addbc1da0ae7ff96a58434dad6b76639\n",
      "Erro ao obter dados do aeroporto ANC: 404 Client Error: Not Found for url: https://airportdb.io/api/v1/airport/KANC?apiToken=e1974b7ce50088c4bf8c096e8722010381ebe6e8aef06041cb3cb2901699c023addbc1da0ae7ff96a58434dad6b76639\n",
      "Erro ao obter dados do aeroporto STT: 404 Client Error: Not Found for url: https://airportdb.io/api/v1/airport/KSTT?apiToken=e1974b7ce50088c4bf8c096e8722010381ebe6e8aef06041cb3cb2901699c023addbc1da0ae7ff96a58434dad6b76639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 11:32:26 WARN TaskSetManager: Stage 84 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:32:36 WARN TaskSetManager: Stage 85 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+\n",
      "|airport_cod|      latitude_deg|     longitude_deg|\n",
      "+-----------+------------------+------------------+\n",
      "|        LGA|         40.777199|        -73.872597|\n",
      "|        EWR|         40.692501|        -74.168701|\n",
      "|        JFK|         40.639801|          -73.7789|\n",
      "|        PSE|              NULL|              NULL|\n",
      "|        MSY| 29.99340057373047|-90.25800323486328|\n",
      "|        SNA|         33.675701|       -117.867996|\n",
      "|        BUR|         34.197703|       -118.356378|\n",
      "|        GRR|       42.88079834|      -85.52279663|\n",
      "|        MYR|     33.6796989441|    -78.9282989502|\n",
      "|        GSO|36.097801208496094|-79.93730163574219|\n",
      "|        PVD|         41.732601|        -71.420403|\n",
      "|        OAK|         37.721298|       -122.221001|\n",
      "|        MSN|           43.1399|        -89.337502|\n",
      "|        DCA|           38.8521|        -77.037697|\n",
      "|        LEX|  38.0364990234375|-84.60590362548828|\n",
      "|        ORF| 36.89459991455078|-76.20120239257812|\n",
      "|        CRW| 38.37310028076172|-81.59320068359375|\n",
      "|        SAV|       32.12760162|      -81.20210266|\n",
      "|        CMH|         39.998001|        -82.891899|\n",
      "|        CAK|40.916099548339844|-81.44219970703125|\n",
      "+-----------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "df = df.withColumn(\"time_hour\", F.to_timestamp(\"time_hour\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# códigos de aeroportos únicos\n",
    "origin_df = df.select(\"origin\").distinct()\n",
    "dest_df = df.select(\"dest\").distinct()\n",
    "\n",
    "airports_cod_df = origin_df.union(dest_df).distinct().withColumnRenamed(\"origin\", \"airport_cod\")\n",
    "\n",
    "# enriquecendo o DataFrame com latitudes e longitudes dos aeroportos\n",
    "latlon_airport_data_df = enrich_airport_data(spark, airports_cod_df, airportdb_key)\n",
    "\n",
    "latlon_airport_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 11:33:16 WARN TaskSetManager: Stage 96 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:33:32 WARN TaskSetManager: Stage 97 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|airport_cod|      date|\n",
      "+-----------+----------+\n",
      "|        JFK|2013-12-06|\n",
      "|        EWR|2013-12-25|\n",
      "|        EWR|2013-02-10|\n",
      "|        LGA|2013-02-17|\n",
      "|        LGA|2013-10-02|\n",
      "|        EWR|2013-04-11|\n",
      "|        LGA|2013-04-16|\n",
      "|        EWR|2013-05-25|\n",
      "|        EWR|2013-06-02|\n",
      "|        LGA|2013-08-17|\n",
      "|        EWR|2013-09-11|\n",
      "|        JFK|2013-09-29|\n",
      "|        JFK|2013-01-06|\n",
      "|        EWR|2013-01-14|\n",
      "|        LGA|2013-10-28|\n",
      "|        LGA|2013-06-16|\n",
      "|        LGA|2013-06-21|\n",
      "|        LGA|2013-07-25|\n",
      "|        LGA|2013-07-31|\n",
      "|        JFK|2013-08-03|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtendo aeroportos únicos por data para origem e destino\n",
    "origin_df = get_unique_airports_by_date(df, \"origin\")\n",
    "dest_df = get_unique_airports_by_date(df, \"dest\")\n",
    "unique_airports_by_date_df = origin_df.union(dest_df).distinct()\n",
    "\n",
    "unique_airports_by_date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 11:34:05 WARN TaskSetManager: Stage 105 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:34:19 WARN TaskSetManager: Stage 106 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:34:32 WARN TaskSetManager: Stage 107 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:34:44 WARN TaskSetManager: Stage 108 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 117:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-------------+\n",
      "|airport_cod|      date|latitude_deg|longitude_deg|\n",
      "+-----------+----------+------------+-------------+\n",
      "|        LGA|2013-09-04|   40.777199|   -73.872597|\n",
      "|        LGA|2013-04-10|   40.777199|   -73.872597|\n",
      "|        LGA|2013-11-08|   40.777199|   -73.872597|\n",
      "|        LGA|2013-09-15|   40.777199|   -73.872597|\n",
      "|        LGA|2013-06-03|   40.777199|   -73.872597|\n",
      "|        LGA|2013-05-28|   40.777199|   -73.872597|\n",
      "|        LGA|2013-03-04|   40.777199|   -73.872597|\n",
      "|        LGA|2013-09-08|   40.777199|   -73.872597|\n",
      "|        LGA|2013-07-26|   40.777199|   -73.872597|\n",
      "|        LGA|2013-09-17|   40.777199|   -73.872597|\n",
      "|        LGA|2013-09-11|   40.777199|   -73.872597|\n",
      "|        LGA|2013-05-17|   40.777199|   -73.872597|\n",
      "|        LGA|2013-04-03|   40.777199|   -73.872597|\n",
      "|        LGA|2013-02-23|   40.777199|   -73.872597|\n",
      "|        LGA|2013-02-13|   40.777199|   -73.872597|\n",
      "|        LGA|2013-01-11|   40.777199|   -73.872597|\n",
      "|        LGA|2013-07-13|   40.777199|   -73.872597|\n",
      "|        LGA|2013-02-28|   40.777199|   -73.872597|\n",
      "|        LGA|2013-10-24|   40.777199|   -73.872597|\n",
      "|        LGA|2013-06-24|   40.777199|   -73.872597|\n",
      "+-----------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# unindo dados de lat/lon com aeroportos por data\n",
    "airport_data_df = unique_airports_by_date_df.join(latlon_airport_data_df, on=\"airport_cod\", how=\"inner\")\n",
    "\n",
    "airport_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enriquecendo o DataFrame com dados climáticos (velocidade do vento)\n",
    "final_airport_data_df = enrich_weather_data(spark, airport_data_df, weatherbit_key)\n",
    "\n",
    "# DataFrame com valores não nulos na coluna 'wind_spd'\n",
    "non_null_wind_spd_df = final_airport_data_df.filter(F.col(\"wind_spd\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame com campo 'date'\n",
    "df = df.withColumn(\"date\", F.concat(F.col(\"year\"), F.lit(\"-\"), F.col(\"month\"), F.lit(\"-\"), F.col(\"day\")))\n",
    "df = df.withColumn(\"date\", F.to_date(\"date\", \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join para obter o 'wind_spd_origin'\n",
    "df_enriched = df.alias(\"o\").join(\n",
    "    non_null_wind_spd_df.drop(\"latitude_deg\", \"longitude_deg\").alias(\"n\"),\n",
    "    (F.col(\"o.date\").cast(StringType()) == F.col(\"n.date\").cast(StringType())) &\n",
    "    (F.col(\"o.origin\").cast(StringType()) == F.col(\"n.airport_cod\").cast(StringType())),\n",
    "    \"left\"\n",
    ").drop(F.col(\"n.date\"), F.col(\"n.airport_cod\"))\n",
    "\n",
    "df_enriched = df_enriched.withColumn(\"wind_spd_origin\", F.col(\"n.wind_spd\")).drop(F.col(\"n.wind_spd\"))\n",
    "\n",
    "# join para obter o 'wind_spd_dest'\n",
    "df_enriched = df_enriched.alias(\"o\").join(\n",
    "    non_null_wind_spd_df.drop(\"latitude_deg\", \"longitude_deg\").alias(\"n\"),\n",
    "    (F.col(\"o.date\").cast(StringType()) == F.col(\"n.date\").cast(StringType())) &\n",
    "    (F.col(\"o.dest\").cast(StringType()) == F.col(\"n.airport_cod\").cast(StringType())),\n",
    "    \"left\"\n",
    ").drop(F.col(\"n.date\"), F.col(\"n.airport_cod\"))\n",
    "\n",
    "df_enriched = df_enriched.withColumn(\"wind_spd_dest\", F.col(\"n.wind_spd\")).drop(F.col(\"n.wind_spd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 11:35:00 WARN TaskSetManager: Stage 126 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:35:07 WARN TaskSetManager: Stage 127 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:35:14 WARN TaskSetManager: Stage 128 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:35:17 WARN TaskSetManager: Stage 129 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/10/09 11:35:22 WARN TaskSetManager: Stage 130 contains a task of very large size (3929 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Erro 429 recebido. Parando novas requisições.                       (0 + 1) / 1]\n",
      "24/10/09 11:35:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_enriched.write.mode(\"overwrite\").option(\"header\", \"true\").parquet(\"../data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
